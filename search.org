#+TITLE: Search
#+PROPERTY: header-args:python :tangle "src/search.py" :mkdirp yes

This is a lesson on searching, or a problem in which an *agent* is given an *initial state* and a *goal state* , and it has to find a specific *set of actions* to reach the latter from the former.

* Essential Terminology

- Agent - an entity(usually a program) that is aware of its environment, perceives it a certain way and acts upon it
- State - "state" of *environment* and the *agent* itself
- Initial state - state in which the agent "begins" - where the *Search algorithm* starts
- Actions - choices we can make in any given state. We'll define a function "*ACTION(s)*" which returns a set of actions that can be executed in state *s*
- Transition model - we can define it as a function "*RESULT( s, a )*" which returns a resulting state, after performing action *a* on the state *s*
- State space - a set of all possible( reachable ) states from the *initial state* by any *sequence of actions*
- Goal test - test whether a given state is the *goal state*
- Path cost function - a number which describes the cost of taking a certain path. Usually we want the *optimal* (shortest) path, which is why we aim to find the shortest one if possible

* Data structures
Here we will declare data structures that will be used in the *Search problem*
** Node

    A simple data structure that will keep track of:

    - Current state
    - Parent node (where we came from)
    - An action which was applied to a parent node to get to the current one
    - A path cost - a number which describes the "cost" of a path( bigger means worse ) from initial state to the current one.

** Frontier

    A frontier is a simple container where we will initially store the *initial state*, and then, after we move from one state to another, we shall store in it states that can be reached from the *current state* by applying a certain *action*

** Explored set

    Explored set is where we keep track of *nodes we have visited* , so we don't end up in an *infinite loop* !

** Stack

    We will use stacks( *LIFO* ) for removing nodes from the *frontier* - we will incorporate it with the *DFS* algorithm.

** Queues

    We will use queues( *FIFO* ) for removing nodes from the *frontier* - we will incorporate it with the *BFS* algorithm

* Initial Approach

The approach to solving a *Search problem* is the following:

    1. Start with a *frontier* that contains only *the initial state*
    2. Initialize the empty explored set
    3. Repeat the following:
       a. If the frontier is *empty*, then there is *no solution*
       b. Remove a node from the frontier - *IMPORTANT FOR OPTIMIZATION!* we can't choose randomly if we want an optimal solution. We will use *BFS* or *DFS* strategies to choose the optimal solution.
       c. If node contains a goal state - return the *solution*
       d. Add the node to the *explored set*
       e. *Expand the node* - add nodes we have "discovered" to the *frontier* ( if they already aren't in the frontier or *the explored set* )

** Depth First Search

    As mentioned before, this search algorithm utilizes *stack data structure( LIFO )*.
    It goes /as deep as possible/ in one direction( in a *tree DS*, it goes out to the *leaves* ), before trying out other paths later.

    Pros:
        - The best-case scenario is: this algorithm is the fastest if luck plays in its favor
    Cons:
        - The solution it finds *isn't guaranteed* to be optimal!
        - At worst - it explores every possible path before it finds the solution, thus taking the longest time

** Breadth First Search

    As mentioned before, this search algorithm utilizes *queue data structure( FIFO )*.
    It first explores the /"neighbouring"/ nodes (or one level away), then moves onto the next level.
    Pros:
        - Guaranteed to find optimal solution - WHY?
    Cons:
        - Runs longer than minimal runtime - almost guaranteed
        - At worst, just as *DFS* , takes longest time to run.

* Code

#+begin_src python
import sys

class Node():
    def __init__(self, state, parent, action):
        self.state   = state
        self.parent  = parent
        self.action  = action

class StackFrontier():
    def __init__(self):
        self.frontier = []

    def add(self, node):
        self.frontier.append(node)

    def contains_state(self,state):
        return any(node.state == state for node in self.frontier)

    def empty(self):
        return len(self.frontier) == 0

    def remove(self):
        if self.empty():
            raise Exception("empty frontier")
        else:
            node = self.frontier[-1]
            self.frontier = self.frontier[:-1]
            return node

class QueueFrontier(StackFrontier):
    def remove(self):
        if self.empty():
            raise Exception("empty frontier")
        else:
            node = self.frontier[0]
            self.frontier = self.frontier[1:]
            return node

class Maze():
    def __init__(self, filename): # ime fajla se dobija iz kompajlerske naredbe =python3 what_to_compile args...=
        # Read file and set height and width of maze
        with open(filename) as f:
            contents = f.read() # procita ceo maze fajl

        # Validate start and goal - can't have more than one start or endpoint
        if contents.count("A") != 1:
            raise Exception("maze must have exactly one start point")
        if contents.count("B") != 1:
            raise Exception("maze must have exactly one goal")

        # Determine height and width of maze
        contents = contents.splitlines() # splitplines uzima ceo sadrzaj "contents"(maze file) i pravi niz recenica koje prelama na osnovu line break-a
        self.height = len(contents) # broj recenica(redova u dokumentu) = visini maze-a
        self.width = max(len(line) for line in contents) # visina maze-a je najduza "recenica" u nizu contents(1 karakter = 1 "kolona")

        # Keep track of walls
        self.walls = [] # self.walls - class field of Maze, it's a matrix which translates a character into True(if character is #)/False(everything else)
        for i in range(self.height): # iterate through every row
            row = [] # keep track of walls for particular row (or index i)
            for j in range(self.width): # iterate through every "column" in that particular "row", effectively making i,j counters a "field" in the maze
                try:
                    if contents[i][j] == "A":
                        self.start = (i, j) # this is where we define the starting point for the Maze using class field "self.start"
                        row.append(False) # we also append False to the current row because starting point isn't a wall
                    elif contents[i][j] == "B":
                        self.goal = (i, j) # this is where we define the end point for the Maze using class field "self.goal"
                        row.append(False) # we also append False to the current row because goal point isn't a wall
                    elif contents[i][j] == " ":
                        row.append(False) # "empty" cells( spaces ) are paths where we can move, so not walls
                    else:
                        row.append(True) # everything else( we're only left with "#" ) is a wall
                except IndexError:
                    row.append(False)
            self.walls.append(row) # every time we complete a row, append it to the "matrix" self.walls

        self.solution = None # initialize the solution field


    def print(self): # just prints out the current state of the Maze
        # solution is an ordered pair of values (actions, states), first being a list of actions taken, second being a list of states/cells visited
        solution = self.solution[1] if self.solution is not None else None
        print()
        for i, row in enumerate(self.walls):
            for j, col in enumerate(row):
                if col:
                    print("â–ˆ", end="")
                elif (i, j) == self.start:
                    print("A", end="")
                elif (i, j) == self.goal:
                    print("B", end="")
                elif solution is not None and (i, j) in solution:
                    print("*", end="")
                else:
                    print(" ", end="")
            print()
        print()


    def neighbors(self, state): # determines neighbours of the given state - state is a pair of numbers (row, column) (not a tuple!)
        row, col = state # which is why we can extract it like this
        candidates = [
            ("up", (row - 1, col)),
            ("down", (row + 1, col)),
            ("left", (row, col - 1)),
            ("right", (row, col + 1))
        ]

        result = []
        for action, (r, c) in candidates:
            if 0 <= r < self.height and 0 <= c < self.width and not self.walls[r][c]:
                # action is viable IFF:
                # NOTE we move into an empty space(not a wall) ^ - reminder: walls is a True/False matrix, where 1 cell says whether it's a wall or not
                # NOTE we don't "fall out" of the map/matrix horizontally, so if the row-value isn't negative or greater than the height of the maze
                # NOTE we don't "fall out" of the map/matrix vertically, so if the column-value isn't negative or greater than the width of the maze
                result.append((action, (r, c)))
        return result


    def solve(self):
        """Finds a solution to maze, if one exists."""

        # Keep track of number of states explored
        self.num_explored = 0

        # Initialize frontier to just the starting position
        start = Node(state=self.start, parent=None, action=None)
        frontier = StackFrontier()
        frontier.add(start)

        # Initialize an empty explored set
        self.explored = set() # keep track of states we have explored so we don't end up in a loop!

        # Keep looping until solution found
        while True:

            # If nothing left in frontier, then no path
            if frontier.empty():
                raise Exception("no solution")

            # Choose a node from the frontier
            node = frontier.remove()
            self.num_explored += 1

            # If node is the goal, then we have a solution
            if node.state == self.goal:
                actions = []
                cells = []
                while node.parent is not None: # if it's none, it means we've reached the start node
                    actions.append(node.action)
                    cells.append(node.state) # cell = state
                    node = node.parent # move upwards the hierarchy through parent pointer
                actions.reverse() # the actions we took in order
                cells.reverse() # the "cells" of a matrix we visited in order
                self.solution = (actions, cells)
                return

            # Mark node as explored
            self.explored.add(node.state)

            # Add neighbors to frontier
            for action, state in self.neighbors(node.state): # neighbors returns a list of ordered pairs (action, state/cell we can go to)
                if not frontier.contains_state(state) and state not in self.explored:
                    # IFF the state isn't already in the frontier(no need for duplicates) and if we haven't explored it already ( to avoid loops ),
                    # only then perform the following:
                    child = Node(state=state, parent=node, action=action) # create a Node we can visit with an action
                    frontier.add(child) # append it to the frontier


    def output_image(self, filename, show_solution=True, show_explored=False):
        from PIL import Image, ImageDraw

        cell_size = 50
        cell_border = 2

        # Create a blank canvas
        img = Image.new(
            "RGBA",
            (self.width * cell_size, self.height * cell_size),
            "black"
        )
        draw = ImageDraw.Draw(img)

        solution = self.solution[1] if self.solution is not None else None
        for i, row in enumerate(self.walls):
            for j, col in enumerate(row):

                # Walls
                if col:
                    fill = (40, 40, 40)

                # Start
                elif (i, j) == self.start:
                    fill = (255, 0, 0)

                # Goal
                elif (i, j) == self.goal:
                    fill = (0, 171, 28)

                # Solution
                elif solution is not None and show_solution and (i, j) in solution:
                    fill = (220, 235, 113)

                # Explored
                elif solution is not None and show_explored and (i, j) in self.explored:
                    fill = (212, 97, 85)

                # Empty cell
                else:
                    fill = (237, 240, 252)

                # Draw cell
                draw.rectangle(
                    (
                        ( (j * cell_size + cell_border, i * cell_size + cell_border),
                          ( (j + 1) * cell_size - cell_border, (i + 1) * cell_size - cell_border) )
                    ),
                    fill=fill
                )

        img.save(filename)

if len(sys.argv) != 2:
    sys.exit("Usage: python maze.py maze.txt")

m = Maze(sys.argv[1]) # sys.argv is array of arguments we pass into the compiler: python3 file-to-compile args...
print("Maze:")
m.print() # prints the entire maze
print("Solving...")
m.solve() # solves the maze
print("States Explored:", m.num_explored) # prints the number of states explored (we keep track of that)
print("Solution:")
m.print() # prints the solution somehow (?)
m.output_image("maze.png", show_explored=True) # prints the solution image
#+end_src

*  Improved approach

    In the improved method we shall use what is known as *informed  search*. The previous two: *BFS /and/ DFS* are known as *uninformed search* strategies, because they don't utilize available *information from the surroundings* to improve the search itself, that is, they are not very perceptive of their environment.

    Informed searches, however, utilize this information to their advantage - to achieve a more optimal solution, or in our case - a *faster one*.
    The implementation provided above isn't sufficient for it to be an informed search, because they use the *LIFO /and/ FIFO* data structures, inserting and reading data in no particular order, other than the order defined by the data structure itself - *this is inefficient* because if we *randomly insert* elements into DS, but read them in a particular order( *LIFO/FIFO* ), we wouldn't be choosing the most optimal path, but *a random one*.

    To choose the most optimal, as we have said, we have to look at our *surroundings* and utilize available information to our advantage. One particular information available to us in the search problem of a *maze* is something what is known as *the Manhattan distance* or *how far away are we from the goal*. "/How far away/" is counted in "/how many blocks horizontally *and/or* vertically/" are we away - *we cannot go diagonally!*

** Greedy Best-First Search Algorithm

    This algorithm is the first example of the /informed search strategy/ we will look at.
    Namely, it *expands* (picks) a node that is the *closest* to the goal, which is determined by a *heuristic function h(n)* - iin our case is going to be the beforementioned /Manhattan distance/ . It's worth noting that Manhattan distance *ignores* /walls/, and only allows movement in a /non-diagonal way/.

    However, even with an informed search such as *GBFS*, it still /doesn't guarantee/ an *optimal solution*.
    Why? Well, if we only expand nodes that /appear/ closest to the goal state, then we only follow what seems to be the best path at any particular moment, which doesn't mean it won't, down the road, turn out to be less-efficient and longer...

    Imagine the following scenario, you are given a choice between two paths:
        1. Which leads to the *left*, and is therefore *1 Manhattan distance* further away from the goal state;
        2. Which leads to the *right*, and is therefore *1 Manhattan distance* /closer/ to the goal state;
    Well, if we know *GBFS* is /always/ going to choose (what appears to be) the /closest/ path, of course it's going to the right path...but what if that right path starts going around in circles, and down the road it turns out to be longer, than if we took the left? What if left only took 1 bad step(left), but later corrected itself, and turned out to be shorter than the right one? That is the disadvantage of this approach, which is why this algorithm has /"greedy"/ in the name.
    This is why we will use an even better algorithm, which utilizes a better *heuristic function* - /A* search algorithm./

** A* search algorithm

    Previous algorithm - *GBFS* utilized only a *heuristic function* - /h(n)/ - which was /the Manhattan distance/, or how far away is a given *state* from the *goal state*.
    What /A*/ does better is it adds additional parameter to the *heuristic* - /g(n)/, which represents the *accumulated* cost from the /initial state/ to the /current state/. So in total, our *heuristic* currently looks like: /f(n) = h(n) + g(n)/, where /h(n)/ is our previous /Manhattan distance/ and /g(n)/ is how many steps in total did it take to get to the current state.
    And now, the performance hog which existed in the previous algorithm is now fixed - once the *current* cost exceeds the /estimated/ cost of other available options, the algorithm will simply switch to them, instead of blindly following the /estimated distance form the goal state/ like h(n) did.

    Now the posed question is, how optimal and efficient is this algorithm? As with any informed search - it is *only* as good as its /heuristic function/.
    It is quite /possible/ that this algorithm can be less efficient than the previous one, or even the uninformed searches, but for it to be optimal, the /heuristic function/ *needs* to be:
        1. *Admissible* - which means it *never* /overestimates/ the true cost, or rather, the *estimated* cost is  *less* or /equal/ to the real one.
        2. *Consistent* - for every node /n/ and its *successor* /n'/ with *step cost* /c/, the following has to apply: /h(n) <= h(n') + c/. Which means, my heuristic needs to be *consistent* accross all the steps that I take, or - the cost of getting to the goal ( /h(n)/ ) from the current node /n/ must be less/equal to the same cost of one of /n/'s *successors* plus the *cost of transitioning* from /n/ to /n'/. If the algorithm *is consistent* then it implies it's also *admissible*.

    This algorithm uses a lot of memory.

* Adversarial search

    In *adversarial searches* an algorithm faces an /"opponent"/ that tries to achieve the goal opposite of the algorithm itself, sort of a friend-or-foe situation, that's why it's often used in games such as *tic-tac-toe*.
    One algorithm that we well observe here is the *Minimax* algorithm.

** Minimax

    We are going to take a look at a game of tic-tac-toe for this one. 2 players, *adversaries*, one player (let's call him *A*) is trying to win, and the other one (call him *B*) is trying to prevent the other one *from* winning. If *A wins* - we'll mark the win as *1*, if *B wins* - it will be *-1*, and if neither win it's a tie - or a *0*. In other words - player *A* is trying to *MAXIMIZE* the score, whislt the player *B* is trying to *MINIMIZE* it(hence the name - /MINIMAX/).

    For our internal representation of the game, we'll need the following:
        1. *The initial state* - we'll mark it with /S0/. In tic-tac-toe it's an *empty game board*.
        2. *Player(s*) function - returns *which* player has the turn in a given state /s/ (player *A* or player *B*). Looks at the game board and determines this (probably checks who made more moves, whoever made less is on turn)
        3. *Action(s)* - returns legal moves in state /s/. Returns a set of possible /"actions"/ .
        4. *Result(s,a)* - returns state after action /a/ is taken in state /s/
        5. *Terminal(s)* - checks if state /s/ is a terminal state
        6. *Utility(s)* - gives us a numerical value for the terminal state /s/

    In the *initial state* our game board is empty - no one made a move, it's the *root* of our tree. (someone at some point makes the first move)
    There's 9 possible initial moves (for whichever player comes first - for example player *A*). And each one of those 9 moves has it's *child* /"moves"/ ( /nodes/ ), these *child moves* belong to the enemy(the other player - *B* for example). Then each and every one of those *children (enemy moves)* has its children(which are now the *A* player moves) and so on and so on...until we reach the *leaves* - which represent the *final possible states*.

    So each "*level*" in a tree is essentially a number of possible options during any *player's turn*. And at each and every one of their turns, the players expand the tree to see "if I make this move, what can my *enemy do*?". In this process, the tree is expanded to it's /leaves/, and each /leaf/ is assigned a number representing the *RESULT* of the game finish.
    Starting from the leaf, and going upwards the tree hierarchy, to the current state, each /node/ is then assigned a number - based on whose turn is it, the player who *MINIMIZES* or *MAXIMIZES*, and also the numbers of their children.
    [[https://cs50.harvard.edu/ai/2020/notes/0/minimax_tictactoe.png][Refer to this picture for the following example:]]

        Say if we have 5 leaves with the following values:
        (1),                    (0),                (-1),               (0),                    (1) <- let's assume that the previous player went for the *MAX* value, what would it pick?
         /                          /                       \                    \                      /
         MAX(1)         MAX(0)                  \                 MAX(0)            /    <--- well, if this  was *MAX*'s move, before that it was *MIN*'s move!
            \                   /                            \                    /                 /
                MIN(0)                                      MIN(-1)                    /        <--- before that it was *MAX*'s turn again!
                    \                                                  /                        /
                        ------------------MAX(1)----------------------

    From this silly representation of a tree(turned 180 degrees) we can see how this works, we have one tree for searching, each level is one player's turn - this also means each level's number is determined by who the player is( whether we're *MAXING* or *MINIMIZING* the score ), and also the values of all of that node's direct children ( we pick the *MAX/MIN* among them ).Those /"children"/ get the scores from *their children* and so on until we reach the *leaf nodes*. So we're sort of alternating between *MINIMIZING* and *MAXIMIZING* between each player's turn (tree level).
    Each of these tree nodes are returned by recursively calling *ACTION(S)* and *RESULT(s,a)* from the current state /s/.
    One thing to keep in mind, we're considering the *POSSIBLE FUTURE* states, it isn't guaranteed that one player will choose a particular *ACTION* to lead to a *STATE* we imagined would be best for us. This would happen if AI played versus a human.

*** Pseudocode

    1. Given a state /s/:
       a. *MAX* picks action /a/ in *ACTIONS(s)* that produces the *highest* value of *MIN-VALUE(RESULT(s,a))*. This means
       b. *MIN* picks action /a/ in *ACTIONS(s)* that produces the *smallest* value of *MAX-VALUE(RESULT(s,a))*. This means

#+BEGIN_EXAMPLE :tangle no
function MAX-VALUE(state):
    if TERMINAL(state):
        return UTILITY(state)
    v = -infinite
    for action in ACTONS(state):
        v = MAX(v, MIN-VALUE( RESULT ( state, action )))
    return v
#+END_EXAMPLE



* Optimizations

    Because the original Minimax takes a lot of space, we need to find a way to optimize it (both space and time)
    To do the required optimization, we'll use something called *Alpha-Beta pruning* and *Depth limited Minimax*.

** Alpha-Beta pruning

    This technique essentially relies on "pruning" unwanted branches of the tree, the ones which almost guarantee will have unfavorable choice for the current player. How do we find this out?
    Let's suppose it's *MAX* player's turn (player *A*), and he's faced with 3 options that *MIN* player (player *B*) /could/ make in the next turn.
    Each of these 3 options were selected based upon what, again, player *A* ( *MAX* ) /could/ do when player *B* makes a hypothetical move.
    Say we have the following [[https://cs50.harvard.edu/ai/2020/notes/0/alphabeta.png][situation]]:

                                                                                                                       4                        <--- this is the player *A* ( *MAX* )
                                                                                                          /            |            \
                                                                                                        4            3            2          <--- this is the player *B* ( *MIN* )
                                                                                                    /   |   \     /  |  \       /  |  \
                                                                                                4      8    5 9  3   X   2   X    X   <--- this is the player *A* again ( *MAX* )

    From this we can notice we have 3 groups:
        1. The last layer has 3 groups, each with 3 options, it's a branch that chose *MAX* values:
           a. The first branch has the numbers ( 4, 8, 5 )
           b. The second is the numbers ( 9, 3 and unknown )
           c. The third one only has 2, and the rest are unknown
        Well, what's with the unknowns? The "/unknowns/" are branches that were *pruded*. Why did we prude *them* specifically? Well, we know that from the first group we'll get *4* (because it's *MIN*'s turn, and well, 4 is the smallest of the 3 numbers). From a perspective of a *MIN* player, 4 is *THE WORST* it could do( 2nd layer ), but from the perspective of the *MAX* player, it's currently *THE BEST POSITION* it can be in! So, if *all the other* branches have a number *<4* associated with them, we can *prune* them, because numbers *less than* the biggest one would put the player *A* ( *MAX* ) in a worse position! (vice-versa for the player *B* ). There's no need to traverse the entire hierarchy of the tree, if we know that the player *A* notices a *3* in the second group, even there *may be a smaller value* (2 or 1 even), 3 is *already* less than 4, and that would put player *A* in a bad spot, there is no need to continue the traversal of that *subtree*. Same thing with the 3rd group, luckily on the first try we notice that the number is 2, which is *even worse* than group 2

** Depth-limited Minimax

    We basically limit how *deep* (after a certain number of moves) the Minimax can go in the tree hierarchy.
    But what do we do if the game still isn't over?

    To do this we'll need to add another piece of terminology called *evaluation function* which is basically a function that /estimates/ the expected utility of the game from a given state, or in other words, assigns values to states which try to /describe/ how favorable a specific *state* is for the player.
    This function is *essential* for a properly functioning AI in practice, because exploring the entire tree of a game like *chess* wouldn't be possible!
